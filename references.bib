@misc{christiano2023deep,
      title={Deep reinforcement learning from human preferences}, 
      author={Paul Christiano and Jan Leike and Tom B. Brown and Miljan Martic and Shane Legg and Dario Amodei},
      year={2023},
      eprint={1706.03741},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      howpublished={\url{https://arxiv.org/abs/1706.03741}}
}

@article{vaswani2017attention,
  title="Attention is all you need",
  author="Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia",
  journal="Advances in Neural Information Processing Systems",
  volume="30",
  year="2017"
}

@article{jumper2021highly,
  title="Highly accurate protein structure prediction with AlphaFold",
  author="Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and Zidek, Augustin and Potapenko, Anna and others",
  journal="Nature",
  volume="596",
  number="7873",
  pages="583--589",
  year="2021",
  publisher="Nature Publishing Group UK London"
}

@inproceedings{liu2021swin,
  title="Swin transformer: Hierarchical vision transformer using shifted windows",
  author="Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining",
  booktitle="Proceedings of the IEEE/CVF international conference on computer vision",
  pages="10012--10022",
  year="2021"
}

@article{brown2020language,
  title="Language models are few-shot learners",
  author="Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others",
  journal="Advances in Neural Information Processing Systems",
  volume="33",
  pages="1877--1901",
  year="2020"
}

@article{ho2020denoising,
  title="Denoising diffusion probabilistic models",
  author="Ho, Jonathan and Jain, Ajay and Abbeel, Pieter",
  journal="Advances in Neural Information Processing Systems",
  volume="33",
  pages="6840--6851",
  year="2020"
}

@misc{fuchs2020se3,
    title="SE(3)-Transformers: 3D roto-translation equivariant attention networks",
    author="Fuchs, Fabian B. and Worrall, Daniel E. and Fischer, Volker and Welling, Max",
    journal="arXiv preprint arXiv:2006.10503",
    howpublished="\url{https://arxiv.org/abs/2006.10503}",
    year="2020"
}

@article{zaheer2020big,
  title="Big bird: Transformers for longer sequences",
  author="Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and others",
  journal="Advances in Neural Information Processing Systems",
  volume="33",
  pages="17283--17297",
  year="2020"
}



@misc{radford2019language,
  title="Language models are unsupervised multitask learners",
  author="Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others",
  journal="OpenAI blog",
  volume="1",
  number="8",
  pages="9",
  year="2019",
  howpublished={\url{https://paperswithcode.com/paper/language-models-are-unsupervised-multitask}}
}

@misc{he2020deberta,
  title="Deberta: Decoding-enhanced bert with disentangled attention",
  author="He, Pengcheng and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu",
  howpublished="\url{https://arxiv.org/abs/2006.03654}", 
  journal="arXiv preprint arXiv:2006.03654",
  year="2020"
}

@inproceedings{he2021deberta,
  title="{DeBERTa: Decoding-enhanced BERT with disentangled attention}",
  author="He, Pengcheng and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu",
  booktitle="International Conference on Learning Representations",
  year="2021"
}

@misc{wang2022e5,
  title="Text Embeddings by Weakly-Supervised Contrastive Pre-training",
  author="Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu",
  howpublished="\url{https://arxiv.org/abs/2212.03533}", 
  journal="arXiv preprint arXiv:2212.03533",
  year="2022"
}

@misc{su2022instructor,
  title="One Embedder, Any Task: Instruction-Finetuned Text Embeddings",
  author="Su, Hongjin Su and Shi, Weijia Shi and Kasai, Jungo and Wang, Yizhong and Hu, Yushi and Ostendorf, Mari and Yih, Wen-tau and Smith, Noah A. and  Zettlemoyer, Luke and Yu, Tao",
  howpublished="\url{https://arxiv.org/abs/2212.09741}", 
  journal="arXiv preprint arXiv:2212.09741",
  year="2022"
}

@inproceedings{liu2019mtdnn,
    title = "Multi-Task Deep Neural Networks for Natural Language Understanding",
    author = "Liu, Xiaodong  and
      He, Pengcheng  and
      Chen, Weizhu  and
      Gao, Jianfeng",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1441",
    doi = "10.18653/v1/P19-1441",
    pages = "4487--4496",
}

@misc{aghajanyan2021muppet,
      title={Muppet: Massive Multi-task Representations with Pre-Finetuning}, 
      author={Armen Aghajanyan and Anchit Gupta and Akshat Shrivastava and Xilun Chen and Luke Zettlemoyer and Sonal Gupta},
      year={2021},
      journal="arXiv preprint arXiv:2101.11038",
      howpublished="\url{https://arxiv.org/abs/2101.11038}"
      
}

@misc{cho2014properties,
  title="On the properties of neural machine translation: Encoder-decoder approaches",
  author="Cho, Kyunghyun and Van Merri{\"e}nboer, Bart and Bahdanau, Dzmitry and Bengio, Yoshua",
  howpublished="\url{https://arxiv.org/abs/1409.1259}", 
  journal="arXiv preprint arXiv:1409.1259",
  year="2014"
}

@misc{bommasani2021opportunities,
  title={On the opportunities and risks of foundation models},
  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  journal={arXiv preprint arXiv:2108.07258},
  howpublished="\url{https://arxiv.org/abs/2108.07258}",
  year={2021}
}

@inproceedings{mikolov2010recurrent,
  title="Recurrent neural network based language model",
  author="Mikolov, Tomas and Karafiat, Martin and Burget, Lukas and Cernocky, Jan and Khudanpur, Sanjeev",
  booktitle="Interspeech",
  volume="2",
  pages="1045--1048",
  year="2010"
}

@article{hochreiter1997long,
  title="Long short-term memory",
  author="Hochreiter, Sepp and Schmidhuber, J{\"u}rgen",
  journal="Neural computation",
  volume="9",
  number="8",
  pages="1735--1780",
  year="1997",
  publisher="MIT press"
}

@article{khan2022transformers,
  title="Transformers in vision: A survey",
  author="Khan, Salman and Naseer, Muzammal and Hayat, Munawar and Zamir, Syed Waqas and Khan, Fahad Shahbaz and Shah, Mubarak",
  journal="ACM computing surveys (CSUR)",
  volume="54",
  number="10s",
  pages="1--41",
  year="2022",
  publisher="ACM New York, NY"
}

@misc{noever2020chess,
  title="The chess transformer: Mastering play using generative language models",
  author="Noever, David and Ciolino, Matt and Kalin, Josh",
  howpublished="\url{https://arxiv.org/abs/2008.04057}", 
  journal="arXiv preprint arXiv:2008.04057",
  year="2020"
}

@misc{noorbakhsh2021pretrained,
  title="Pretrained Language Models are Symbolic Mathematics Solvers too!",
  author="Noorbakhsh, Kimia and Sulaiman, Modar and Sharifi, Mahdi and Roy, Kallol and Jamshidi, Pooyan",
  howpublished="\url{https://arxiv.org/abs/2110.03501}", 
  journal="arXiv preprint arXiv:2110.03501",
  year="2021"
}

@misc{yang2022diffusion,
  title="Diffusion models: A comprehensive survey of methods and applications",
  author="Yang, Ling and Zhang, Zhilong and Song, Yang and Hong, Shenda and Xu, Runsheng and Zhao, Yue and Shao, Yingxia and Zhang, Wentao and Cui, Bin and Yang, Ming-Hsuan",
  howpublished="\url{https://arxiv.org/abs/2209.00796}", 
  journal="arXiv preprint arXiv:2209.00796",
  year="2022"
}

@article{qiu2020pre,
  title="Pre-trained models for natural language processing: A survey",
  author="Qiu, Xipeng and Sun, Tianxiang and Xu, Yige and Shao, Yunfan and Dai, Ning and Huang, Xuanjing",
  journal="Science China Technological Sciences",
  volume="63",
  number="10",
  pages="1872--1897",
  year="2020",
  publisher="Springer"
}


@misc{bai2022training,
    title="Training a helpful and harmless assistant with reinforcement learning from human feedback",
    author={Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and Joseph, Nicholas and Kadavath, Saurav and Kernion, Jackson and Conerly, Tom and El-Showk, Sheer and Elhage, Nelson and Hatfield-Dodds, Zac and Hernandez, Danny and Hume, Tristan and Johnston, Scott and Kravec, Shauna and Lovitt, Liane and Nanda, Neel and Olsson, Catherine and Amodei, Dario and Brown, Tom and Clark, Jack and McCandlish, Sam and Olah, Chris and Mann, Ben and Kaplan, Jared},
    journal="arXiv preprint arXiv:2204.05862",
    howpublished="\url{https://arxiv.org/abs/2204.05862}",
    year="2022"
}

@misc{bai2022constitutional,
    title={Constitutional AI: Harmlessness from AI Feedback}, 
    author={Yuntao Bai and Saurav Kadavath and Sandipan Kundu and Amanda Askell and Jackson Kernion and Andy Jones and Anna Chen and Anna Goldie and Azalia Mirhoseini and Cameron McKinnon and Carol Chen and Catherine Olsson and Christopher Olah and Danny Hernandez and Dawn Drain and Deep Ganguli and Dustin Li and Eli Tran-Johnson and Ethan Perez and Jamie Kerr and Jared Mueller and Jeffrey Ladish and Joshua Landau and Kamal Ndousse and Kamile Lukosuite and Liane Lovitt and Michael Sellitto and Nelson Elhage and Nicholas Schiefer and Noemi Mercado and Nova DasSarma and Robert Lasenby and Robin Larson and Sam Ringer and Scott Johnston and Shauna Kravec and Sheer El Showk and Stanislav Fort and Tamera Lanham and Timothy Telleen-Lawton and Tom Conerly and Tom Henighan and Tristan Hume and Samuel R. Bowman and Zac Hatfield-Dodds and Ben Mann and Dario Amodei and Nicholas Joseph and Sam McCandlish and Tom Brown and Jared Kaplan},
    journal="arXiv preprint arXiv:2212.08073",
    howpublished="\url{https://arxiv.org/abs/2212.08073}",
    year={2022}
}

@misc{askell2021general,
  title="A general language assistant as a laboratory for alignment",
  author="Askell, Amanda and Bai, Yuntao and Chen, Anna and Drain, Dawn and Ganguli, Deep and Henighan, Tom and Jones, Andy and Joseph, Nicholas and Mann, Ben and DasSarma, Nova and others",
  journal="arXiv preprint arXiv:2112.00861",
  year="2021",
  howpublished="\url{https://arxiv.org/abs/2112.00861}"
}

@misc{lewis2019bart,
  title="Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
  author="Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke",
  howpublished="\url{https://arxiv.org/abs/1910.13461}", 
  journal="arXiv preprint arXiv:1910.13461",
  year="2019"
}

@misc{devlin2018bert,
  title="Bert: Pre-training of deep bidirectional transformers for language understanding",
  author="Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina",
  howpublished="\url{https://arxiv.org/abs/1810.04805}", 
  journal="arXiv preprint arXiv:1810.04805",
  year="2018"
}

@misc{qinkai2023codegeex,
  title="CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Evaluations on HumanEval-X",
  author="Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen, Andi Wang, Yang Li, Teng Su, Zhilin Yang, Jie Tang",
  howpublished="\url{https://arxiv.org/abs/2303.17568}", 
  journal="arXiv preprint arXiv:2303.17568",
  year="2023"
}

@misc{allal2023santacoder,
  title="SantaCoder: don't reach for the stars!",
  author="Allal, Loubna Ben and Li, Raymond and Kocetkov, Denis and Mou, Chenghao and Akiki, Christopher and Ferrandis, Carlos Munoz and Muennighoff, Niklas and Mishra, Mayank and Gu, Alex and Dey, Manan and others",
  howpublished="\url{https://arxiv.org/abs/2301.03988}", 
  journal="arXiv preprint arXiv:2301.03988",
  year="2023"
}

@misc{victor2022t0,
  title="Multitask Prompted Training Enables Zero-Shot Task Generalization",
  author="Victor Sanh and Albert Webson and Colin Raffel and Stephen H. Bach and Lintang Sutawika, et al.",
  howpublished="\url{https://arxiv.org/abs/2110.08207}", 
  journal="arXiv preprint arXiv:2110.08207",
  year="2021"
}

@misc{alephalpha2019luminous,
  title="What is Luminous?",
  author="Aleph Alpha",
  year="2019",
  howpublished = "\url{https://docs.alephalpha.com/docs/introduction/luminous}"
}

@misc{mark2021codex,
  title="Evaluating Large Language Models Trained on Code",
  author="Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al.",
  howpublished="\url{https://arxiv.org/abs/2107.03374}", 
  journal = "arXiv preprint arXiv:2107.03374",
  year="2021"
}

@misc{zhengxiao2022glm,
  title="GLM: General Language Model Pretraining with Autoregressive Blank Infilling",
  author="Zhengxiao Du and Yujie Qian and Xiao Liu and Ming Ding and Jiezhong Qiu and Zhilin Yang and Jie Tang",
  journal="arXiv preprint arXiv:2103.10360",
  howpublished="\url{https://arxiv.org/abs/2103.10360}",
  year="2022"
}

@misc{taylor2022galactica,
  title="GALACTICA: A Large Language Model for Science",
  author="Ross Taylor and Marcin Kardas and Guillem Cucurull and Thomas Scialom and Anthony Hartshorn and Elvis Saravia and Andrew Poulton and Viktor Kerkez and Robert Stojnic",
  journal="arXiv preprint arXiv:2211.09085",
  howpublished="\url{https://arxiv.org/abs/2211.09085}",
  year="2022"
}

@misc{hyung2022flant5,
  title="Scaling Instruction-Finetuned Language Models",
  author="Hyung Won Chung and Le Hou and Shayne Longpre and Barret Zoph and Yi Tay and William Fedus and Yunxuan Li and Xuezhi Wang and Mostafa Dehghani and Siddhartha Brahma and Albert Webson and Shixiang Shane Gu and Zhuyun Dai and Mirac Suzgun and Xinyun Chen and Aakanksha Chowdhery and Alex Castro-Ros and Marie Pellat and Kevin Robinson and Dasha Valter and Sharan Narang and Gaurav Mishra and Adams Yu and Vincent Zhao and Yanping Huang and Andrew Dai and Hongkun Yu and Slav Petrov and Ed H. Chi and Jeff Dean and Jacob Devlin and Adam Roberts and Denny Zhou and Quoc V. Le and Jason Wei",
  journal="arXiv preprint arXiv:2210.11416",
  howpublished={\url{https://arxiv.org/abs/2210.11416}},
  year="2022"
}

@misc{yi2022ul2,
  title="Unifying Language Learning Paradigms",
  author="Yi Tay and Mostafa Dehghani and Vinh Q. Tran and Xavier Garcia and Dara Bahri and Tal Schuster and Huaixiu Steven Zheng and Neil Houlsby and Donald Metzler",
  howpublished="\url{https://arxiv.org/abs/2205.05131}", 
  journal="arXiv preprint arXiv:2205.05131",
  year="2022"
}

@misc{shuster2022blenderbot,
  title="Blenderbot 3: a deployed conversational agent that continually learns to responsibly engage",
  author="Shuster, Kurt and Xu, Jing and Komeili, Mojtaba and Ju, Da and Smith, Eric Michael and Roller, Stephen and Ung, Megan and Chen, Moya and Arora, Kushal and Lane, Joshua and others",
  journal="arXiv preprint arXiv:2208.03188",
  howpublished="\url{https://arxiv.org/abs/2208.03188}",
  year="2022"
}

@misc{hoffmann2022training,
  title="Training compute-optimal large language models",
  author="Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others",
  howpublished="\url{https://arxiv.org/abs/2203.15556}", 
  journal="arXiv preprint arXiv:2203.15556",
  year="2022"
}

@inproceedings{radford2021learning,
  title="Learning transferable visual models from natural language supervision",
  author="Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others",
  booktitle="International conference on machine learning",
  pages="8748--8763",
  year="2021",
  organization="PMLR"
}

@misc{aghajanyan2022cm3,
  title="Cm3: A causal masked multimodal model of the internet",
  author="Aghajanyan, Armen and Huang, Bernie and Ross, Candace and Karpukhin, Vladimir and Xu, Hu and Goyal, Naman and Okhonko, Dmytro and Joshi, Mandar and Ghosh, Gargi and Lewis, Mike and others",
  howpublished="\url{https://arxiv.org/abs/2201.07520}", 
  journal="arXiv preprint arXiv:2201.07520",
  year="2022"
}

@misc{keskar2019ctrl,
  title="Ctrl: A conditional transformer language model for controllable generation",
  author="Keskar, Nitish Shirish and McCann, Bryan and Varshney, Lav R and Xiong, Caiming and Socher, Richard",
  howpublished="\url{https://arxiv.org/abs/1909.05858}", 
  journal="arXiv preprint arXiv:1909.05858",
  year="2019"
}

@inproceedings{ramesh2021zero,
  title="Zero-shot text-to-image generation",
  author="Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya",
  booktitle="International Conference on Machine Learning",
  pages="8821--8831",
  year="2021",
  organization="PMLR"
}

@misc{ramesh2022hierarchical,
  title="Hierarchical text-conditional image generation with clip latents",
  author="Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark",
  howpublished="\url{https://arxiv.org/abs/2204.06125}", 
  journal="arXiv preprint arXiv:2204.06125",
  year="2022"
}

@article{chen2021decision,
  title="Decision transformer: Reinforcement learning via sequence modeling",
  author="Chen, Lili and Lu, Kevin and Rajeswaran, Aravind and Lee, Kimin and Grover, Aditya and Laskin, Misha and Abbeel, Pieter and Srinivas, Aravind and Mordatch, Igor",
  journal="Advances in Neural Information Processing Systems",
  volume="34",
  pages="15084--15097",
  year="2021"
}

@misc{zhang2019dialogpt,
  title="Dialogpt: Large-scale generative pre-training for conversational response generation",
  author="Zhang, Yizhe and Sun, Siqi and Galley, Michel and Chen, Yen-Chun and Brockett, Chris and Gao, Xiang and Gao, Jianfeng and Liu, Jingjing and Dolan, Bill",
  howpublished="\url{https://arxiv.org/abs/1911.00536}", 
  journal="arXiv preprint arXiv:1911.00536",
  year="2019"
}

@misc{sanh2019distilbert,
  title="DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
  author="Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas",
  howpublished="\url{https://arxiv.org/abs/1910.01108}", 
  journal="arXiv preprint arXiv:1910.01108",
  year="2019"
}

@misc{li2022dq,
  title="DQ-BART: Efficient Sequence-to-Sequence Model via Joint Distillation and Quantization",
  author="Li, Zheng and Wang, Zijian and Tan, Ming and Nallapati, Ramesh and Bhatia, Parminder and Arnold, Andrew and Xiang, Bing and Roth, Dan",
  howpublished="\url{https://arxiv.org/abs/2203.11239}", 
  journal="arXiv preprint arXiv:2203.11239",
  year="2022"
}

@misc{clark2020electra,
  title="Electra: Pre-training text encoders as discriminators rather than generators",
  author="Clark, Kevin and Luong, Minh-Thang and Le, Quoc V and Manning, Christopher D",
  howpublished="\url{https://arxiv.org/abs/2003.10555}", 
  journal="arXiv preprint arXiv:2003.10555",
  year="2020"
}

@misc{zhang2019ernie,
  title="ERNIE: Enhanced language representation with informative entities",
  author="Zhang, Zhengyan and Han, Xu and Liu, Zhiyuan and Jiang, Xin and Sun, Maosong and Liu, Qun",
  howpublished="\url{https://arxiv.org/abs/1905.07129}", 
  journal="arXiv preprint arXiv:1905.07129",
  year="2019"
}

@misc{alayrac2022flamingo,
  title="Flamingo: a visual language model for few-shot learning",
  author="Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katie and Reynolds, Malcolm and others",
  howpublished="\url{https://arxiv.org/abs/2204.14198}", 
  journal="arXiv preprint arXiv:2204.14198",
  year="2022"
}

@misc{reed2022generalist,
  title="A generalist agent",
  author="Reed, Scott and Zolna, Konrad and Parisotto, Emilio and Colmenarejo, Sergio Gomez and Novikov, Alexander and Barth-Maron, Gabriel and Gimenez, Mai and Sulsky, Yury and Kay, Jackie and Springenberg, Jost Tobias and others",
  journal="arXiv preprint arXiv:2205.06175",
  howpublished="\url{https://arxiv.org/abs/2205.06175}",
  year="2022"
}

@inproceedings{du2022glam,
  title="Glam: Efficient scaling of language models with mixture-of-experts",
  author="Du, Nan and Huang, Yanping and Dai, Andrew M and Tong, Simon and Lepikhin, Dmitry and Xu, Yuanzhong and Krikun, Maxim and Zhou, Yanqi and Yu, Adams Wei and Firat, Orhan and others",
  booktitle="International Conference on Machine Learning",
  pages="5547--5569",
  year="2022",
  organization="PMLR"
}

@misc{nichol2021glide,
  title="Glide: Towards photorealistic image generation and editing with text-guided diffusion models",
  author="Nichol, Alex and Dhariwal, Prafulla and Ramesh, Aditya and Shyam, Pranav and Mishkin, Pamela and McGrew, Bob and Sutskever, Ilya and Chen, Mark",
  howpublished="\url{https://arxiv.org/abs/2112.10741}", 
  journal="arXiv preprint arXiv:2112.10741",
  year="2021"
}

@misc{hatamizadeh2022global,
  title="Global context vision transformers",
  author="Hatamizadeh, Ali and Yin, Hongxu and Kautz, Jan and Molchanov, Pavlo",
  howpublished="\url{https://arxiv.org/abs/2206.09959}", 
  journal="arXiv preprint arXiv:2206.09959",
  year="2022"
}

@misc{rae2021scaling,
  title="Scaling language models: Methods, analysis \& insights from training gopher",
  author="Rae, Jack W and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and others",
  howpublished="\url{https://arxiv.org/abs/2112.11446}", 
  journal="arXiv preprint arXiv:2112.11446",
  year="2021"
}

@misc{menick2022teaching,
  title="Teaching language models to support answers with verified quotes",
  author="Menick, Jacob and Trebacz, Maja and Mikulik, Vladimir and Aslanides, John and Song, Francis and Chadwick, Martin and Glaese, Mia and Young, Susannah and Campbell-Gillingham, Lucy and Irving, Geoffrey and others",
  howpublished="\url{https://arxiv.org/abs/2203.11147}", 
  journal="arXiv preprint arXiv:2203.11147",
  year="2022"
}

@misc{radford2018improving,
  title="Improving language understanding by generative pre-training",
  author="Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others",
  year="2018",
  howpublished="\url{https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf}",
  publisher={OpenAI}
}


@misc{ouyang2022training,
  title="Training language models to follow instructions with human feedback",
  author="Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others",
  howpublished="\url{https://arxiv.org/abs/2203.02155}", 
  journal="arXiv preprint arXiv:2203.02155",
  year="2022"
}

@misc{black2022gpt,
  title="Gpt-neox-20b: An open-source autoregressive language model",
  author="Black, Sid and Biderman, Stella and Hallahan, Eric and Anthony, Quentin and Gao, Leo and Golding, Laurence and He, Horace and Leahy, Connor and McDonell, Kyle and Phang, Jason and others",
  howpublished="\url{https://arxiv.org/abs/2204.06745}", 
  journal="arXiv preprint arXiv:2204.06745",
  year="2022"
}

@misc{aghajanyan2021htlm,
  title="Htlm: Hyper-text pre-training and prompting of language models",
  author="Aghajanyan, Armen and Okhonko, Dmytro and Lewis, Mike and Joshi, Mandar and Xu, Hu and Ghosh, Gargi and Zettlemoyer, Luke",
  howpublished="\url{https://arxiv.org/abs/2107.06955}", 
  journal="arXiv preprint arXiv:2107.06955",
  year="2021"
}

@misc{saharia2022photorealistic,
  title="Photorealistic text-to-image diffusion models with deep language understanding",
  author="Saharia, Chitwan and Chan, William and Saxena, Saurabh and Li, Lala and Whang, Jay and Denton, Emily and Ghasemipour, Seyed Kamyar Seyed and Ayan, Burcu Karagol and Mahdavi, S Sara and Lopes, Rapha Gontijo and others",
  howpublished="\url{https://arxiv.org/abs/2205.11487}", 
  journal="arXiv preprint arXiv:2205.11487",
  year="2022"
}

@misc{lieber2021jurassic,
  title="Jurassic-1: Technical details and evaluation",
  author="Lieber, Opher and Sharir, Or and Lenz, Barak and Shoham, Yoav",
  journal="White Paper. AI21 Labs",
  volume="1",
  year="2021",
  howpublished={\url{https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf}}
}

@misc{thoppilan2022lamda,
  title="Lamda: Language models for dialog applications",
  author="Thoppilan, Romal and De Freitas, Daniel and Hall, Jamie and Shazeer, Noam and Kulshreshtha, Apoorv and Cheng, Heng-Tze and Jin, Alicia and Bos, Taylor and Baker, Leslie and Du, Yu and others",
  howpublished="\url{https://arxiv.org/abs/2201.08239}", 
  journal="arXiv preprint arXiv:2201.08239",
  year="2022"
}

@article{liu2020multilingual,
  title="Multilingual denoising pre-training for neural machine translation",
  author="Liu, Yinhan and Gu, Jiatao and Goyal, Naman and Li, Xian and Edunov, Sergey and Ghazvininejad, Marjan and Lewis, Mike and Zettlemoyer, Luke",
  journal="Transactions of the Association for Computational Linguistics",
  volume="8",
  pages="726--742",
  year="2020",
  publisher="MIT Press"
}

@misc{shoeybi2019megatron,
  title="Megatron-lm: Training multi-billion parameter language models using model parallelism",
  author="Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan",
  howpublished="\url{https://arxiv.org/abs/1909.08053}", 
  journal="arXiv preprint arXiv:1909.08053",
  year="2019"
}

@misc{lewkowycz2022solving,
  title="Solving quantitative reasoning problems with language models",
  author="Lewkowycz, Aitor and Andreassen, Anders and Dohan, David and Dyer, Ethan and Michalewski, Henryk and Ramasesh, Vinay and Slone, Ambrose and Anil, Cem and Schlag, Imanol and Gutman-Solo, Theo and others",
  howpublished="\url{https://arxiv.org/abs/2206.14858}", 
  journal="arXiv preprint arXiv:2206.14858",
  year="2022"
}

@misc{smith2022using,
  title="Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model",
  author="Smith, Shaden and Patwary, Mostofa and Norick, Brandon and LeGresley, Patrick and Rajbhandari, Samyam and Casper, Jared and Liu, Zhun and Prabhumoye, Shrimai and Zerveas, George and Korthikanti, Vijay and others",
  howpublished="\url{https://arxiv.org/abs/2201.11990}", 
  journal="arXiv preprint arXiv:2201.11990",
  year="2022"
}

@misc{biderman2023pythia,
  title="Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling",
  author="Stella Biderman and Hailey Schoelkopf and Quentin Anthony and",
  howpublished="\url{https://arxiv.org/abs/2304.01373}", 
  journal="arXiv preprint arXiv:2304.01373",
  year="2023"
}

@misc{zhang2022opt,
  title="Opt: Open pre-trained transformer language models",
  author="Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others",
  howpublished="\url{https://arxiv.org/abs/2205.01068}", 
  journal="arXiv preprint arXiv:2205.01068",
  year="2022"
}

@misc{kopf2022openassistant,
  title="OpenAssistant Conversations -- Democratizing Large Language Model Alignment",
  author="Andreas Köpf and Yannic Kilcher and Dimitri von Rütte and Sotiris Anagnostidis and Zhi-Rui Tam and Keith Stevens and Abdullah Barhoum and Nguyen Minh Duc and Oliver Stanley and Richárd Nagyfi and Shahul ES and Sameer Suri and David Glushkov and Arnav Dantuluri and Andrew Maguire and Christoph Schuhmann and Huu Nguyen and Alexander Mattick",
  howpublished="\url{https://arxiv.org/abs/2304.07327}", 
  journal="arXiv preprint arXiv:2304.07327",
  year="2023"
}

@misc{chowdhery2022palm,
  title="Palm: Scaling language modeling with pathways",
  author="Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others",
  howpublished="\url{https://arxiv.org/abs/2204.02311}", 
  journal="arXiv preprint arXiv:2204.02311",
  year="2022"
}

@inproceedings{zhang2020pegasus,
  title="Pegasus: Pre-training with extracted gap-sentences for abstractive summarization",
  author="Zhang, Jingqing and Zhao, Yao and Saleh, Mohammad and Liu, Peter",
  booktitle="International Conference on Machine Learning",
  pages="11328--11339",
  year="2020",
  organization="PMLR"
}

@misc{liu2019roberta,
  title="Roberta: A robustly optimized bert pretraining approach",
  author="Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin",
  howpublished="\url{https://arxiv.org/abs/1907.11692}", 
  journal="arXiv preprint arXiv:1907.11692",
  year="2019"
}

@misc{shuster2022language,
  title="Language models that seek for knowledge: Modular search \& generation for dialogue and prompt completion",
  author="Shuster, Kurt and Komeili, Mojtaba and Adolphs, Leonard and Roller, Stephen and Szlam, Arthur and Weston, Jason",
  howpublished="\url{https://arxiv.org/abs/2203.13224}", 
  journal="arXiv preprint arXiv:2203.13224",
  year="2022"
}

@misc{glaese2022improving,
  title="Improving alignment of dialogue agents via targeted human judgements",
  author="Glaese, Amelia and McAleese, Nat and Tr{\k{e}}bacz, Maja and Aslanides, John and Firoiu, Vlad and Ewalds, Timo and Rauh, Maribeth and Weidinger, Laura and Chadwick, Martin and Thacker, Phoebe and others",
  howpublished="\url{https://arxiv.org/abs/2209.14375}", 
  journal="arXiv preprint arXiv:2209.14375",
  year="2022"
}

@inproceedings{rombach2022high,
  title="High-resolution image synthesis with latent diffusion models",
  author="Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn",
  booktitle="Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
  pages="10684--10695",
  year="2022"
}


@article{fedus2021switch,
  title="Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity",
  author="Fedus, William and Zoph, Barret and Shazeer, Noam",
  journal="Journal of Machine Learning Research",
  volume="23",
  pages="1--40",
  year="2021"
}

@article{raffel2020exploring,
  title="Exploring the limits of transfer learning with a unified text-to-text transformer",
  author="Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J",
  journal="The Journal of Machine Learning Research",
  volume="21",
  number="1",
  pages="5485--5551",
  year="2020",
  publisher="JMLRORG"
}

@article{janner2021offline,
  title="Offline reinforcement learning as one big sequence modeling problem",
  author="Janner, Michael and Li, Qiyang and Levine, Sergey",
  journal="Advances in Neural Information Processing Systems",
  volume="34",
  pages="1273--1286",
  year="2021"
}

@misc{dai2019transformer,
  title="Transformer-xl: Attentive language models beyond a fixed-length context",
  author="Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V and Salakhutdinov, Ruslan",
  howpublished="\url{https://arxiv.org/abs/1901.02860}", 
  journal="arXiv preprint arXiv:1901.02860",
  year="2019"
}

@misc{rosset2020turing,
  title="Turing-NLG: A 17-billion-parameter language model by Microsoft",
  author="Rosset, Corby",
  journal="Microsoft Blog",
  volume="1",
  number="2",
  year="2020",
  howpublished="\url{https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/}"
}

@misc{dosovitskiy2020image,
  title="An image is worth 16x16 words: Transformers for image recognition at scale",
  author="Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others",
  howpublished="\url{https://arxiv.org/abs/2010.11929}", 
  journal="arXiv preprint arXiv:2010.11929",
  year="2020"
}

@misc{conneau2019unsupervised,
  title="Unsupervised cross-lingual representation learning at scale",
  author="Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzm{\'a}n, Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin",
  howpublished="\url{https://arxiv.org/abs/1911.02116}", 
  journal="arXiv preprint arXiv:1911.02116",
  year="2019"
}

@article{yang2019xlnet,
  title="Xlnet: Generalized autoregressive pretraining for language understanding",
  author="Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V",
  journal="Advances in Neural Information Processing Systems",
  volume="32",
  year="2019"
}

@article{lin2022survey,
  title="A survey of transformers",
  author="Lin, Tianyang and Wang, Yuxin and Liu, Xiangyang and Qiu, Xipeng",
  journal="AI Open",
  year="2022",
  volume="3",
  pages="111-132",
  publisher="Elsevier"
}

@misc{touvron2023llama,
      title="LLaMA: Open and Efficient Foundation Language Models", 
      author="Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample",
      year="2023",
      eprint="2302.13971",
      archivePrefix="arXiv",
      primaryClass="cs.CL",
      howpublished={\url{https://arxiv.org/abs/2302.13971}}
}

@misc{alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford Alpaca: An Instruction-following LLaMA model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}

@misc{lan2019albert,
  title="ALBERT: A lite BERT for self-supervised learning of language representations",
  author={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel  and Piyush Sharma and Radu Soricut},
  journal="arXiv preprint arXiv:1909.11942",
  year="2019",
  howpublished="\url{https://arxiv.org/abs/1909.11942}"
}

@misc{soltan2022alexatm,
      title={AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model}, 
      author={Saleh Soltan and Shankar Ananthakrishnan and Jack FitzGerald and Rahul Gupta and Wael Hamza and Haidar Khan and Charith Peris and Stephen Rawls and Andy Rosenbaum and Anna Rumshisky and Chandana Satya Prakash and Mukund Sridhar and Fabian Triefenbach and Apurv Verma and Gokhan Tur and Prem Natarajan},
      year={2022},
      eprint={2208.01448},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      howpublished="\url{https://arxiv.org/abs/2208.01448}"
}

@misc{gpt-j,
  author = {Wang, Ben and Komatsuzaki, Aran},
  title = {{GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model}},
  howpublished = {\url{https://github.com/kingoflolz/mesh-transformer-jax}},
  year = 2021,
  month = May
}

@article{jo2023promise,
  title={The Promise and Peril of Generative AI},
  author={Jo, A},
  journal={Nature},
  volume={614},
  number={1},
  year={2023}
}

@misc{stokelwalker2023nature,
  title={What ChatGPT and generative AI mean for science},
  author={Chris Stokel-Walker and Richard Van Noorden},
  journal={Nature},
  howpublished = {\url{https://www.nature.com/articles/d41586-023-00340-6}},
  year={2023}
}


@misc{baidoo2023education,
  title={Education in the era of generative artificial intelligence (AI): Understanding the potential benefits of ChatGPT in promoting teaching and learning},
  author={Baidoo-Anu, David and Owusu Ansah, Leticia},
  journal={Available at SSRN 4337484},
  howpublished="\url{https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4337484}",
  year={2023}
}

@misc{UBSChatGPT2023,
    author = {Kevin Dennean and Sundeep Gantori and Delwin Kurnia Limas and Allen Pu,and Reid Gilligan},
    title = {Let's chat about ChatGPT},
    institution = {Chief Investment Office, UBS},
    year = {2023},
    howpublished="\url{https://www.ubs.com/global/en/wealth-management/our-approach/marketnews/article.1585717.html}"
    }

@misc{gpt-4,
    author = {OpenAI},
    title = {GPT-4 Technical Report},
    institution = {OpenAI},
    year = {2023},
    howpublished="\url{https://arxiv.org/abs/2303.08774}"
    }

@inproceedings{esser2021taming,
  title={Taming transformers for high-resolution image synthesis},
  author={Esser, Patrick and Rombach, Robin and Ommer, Bjorn},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={12873--12883},
  year={2021}
}